ğŸŒ± Soil Spectroscopy Prediction
Deep Learning Â· Machine Learning Â· Hyperspectral Analysis

This project predicts five key soil properties using hyperspectral reflectance data combined with environmental tabular features.
It integrates chemometrics, ML models, 1D-CNNs, and a stacked ensemble for high-accuracy multi-target regression.

ğŸ“Œ Project Goals

Predict the following soil attributes using spectroscopy:

SOC (Soil Organic Carbon)

pH

Ca (Calcium)

P (Phosphorus)

Sand %

ğŸ“‚ Repository Structure
â”œâ”€â”€ data/                       # Raw spectral + tabular data
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ DEEP_Learning_Project.ipynb
â”œâ”€â”€ models/                     # Saved models (Autoencoder, CNN, Ensemble)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py        # SG filtering, scaling, PCA
â”‚   â”œâ”€â”€ models.py               # All ML + DL architectures
â”‚   â”œâ”€â”€ stacking.py             # Stacking ensemble
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸš€ Methods Overview
1ï¸âƒ£ Traditional Machine Learning

PLS Regression â€” baseline chemometric model

LightGBM â€” trained on PCA-reduced spectra + statistical aggregates

2ï¸âƒ£ Transfer Learning with 1D Autoencoder

Encoder:

Conv1D layers + MaxPooling

Compresses ~3500 spectral features â†’ 128-dim latent space

Decoder:

Reconstructs spectra (used only during pretraining)

Transfer Step:

Decoder removed

Encoder frozen/fine-tuned + Dense regression head

3ï¸âƒ£ Hybrid Deep Learning Model (Multi-Input)
Branch	Input	Architecture
A â€” 1D-CNN	Spectral data	3Ã—Conv1D â†’ GlobalAveragePooling
B â€” Dense Network	Tabular features	Dense â†’ Dropout
Fusion	concat(A,B)	Dense â†’ Output(5 targets)
4ï¸âƒ£ Stacked Ensemble (Final Model)

Base (Level-0) Learners:

PLS

LightGBM

Hybrid CNN

Meta-Learner (Level-1):

Ridge Regression

â¡ï¸ Best-performing model in the project.

âš™ï¸ Training Details

Frameworks: TensorFlow/Keras, Scikit-learn, LightGBM

Hardware: Google Colab (T4 GPU)

Cross-Validation: 5-Fold

Optimizer: Adam

Loss: MSE

Batch Size: 32

Epochs:

Autoencoder â†’ 10

Hybrid CNN â†’ 80 (Early Stopping)

ğŸ“Š Evaluation
Primary Metric: MCRMSE

Mean Columnwise RMSE across all 5 targets.

Model Performance
Model	Score / Observation
Hybrid CNN	Loss â‰ˆ 0.68
Stacked Ensemble	MCRMSE â‰ˆ 0.438 (Best)
Improvement	~7% better than best single model

Visualization:
Predicted vs. Actual scatter plots show the ensemble gives the tightest fit around the y = x line.

ğŸ§  Key Insights

Deep models need larger datasets â€” CNN alone underperforms with ~1157 samples.

Fusion of spectral + tabular data boosts accuracy.

Autoencoder reduces noise, stabilizes CNN training.

Stacking provides robust error correction across diverse models.

ğŸ”® Future Enhancements

Spectral data augmentation (noise, shifts)

Attention layers for wavelength-level feature focus

Optuna hyperparameter tuning

Use larger public soil spectral libraries

ğŸ‘¥ Contributors

Gautam (102215039)

Navneet (102215082)

Urja (102215084)

Gaureesh (102215127)

Mehak (102215163)

Subgroup: 4NC6
